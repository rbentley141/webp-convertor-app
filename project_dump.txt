# PROJECT DUMP FOR CHATGPT

## Directory tree

```text
WebPForge/
├── apps/
│   ├── backend/
│   │   ├── results/
│   │   ├── src/
│   │   │   └── webp_backend/
│   │   │       ├── routes/
│   │   │       │   ├── __init__.py
│   │   │       │   ├── jobs.py
│   │   │       │   └── uploads.py
│   │   │       ├── services/
│   │   │       │   ├── __init__.py
│   │   │       │   └── job_service.py
│   │   │       ├── __init__.py
│   │   │       ├── app.py
│   │   │       └── config.py
│   │   ├── uploads/
│   │   │   └── 1.zip
│   │   └── pyproject.toml
│   ├── frontend/
│   │   ├── public/
│   │   │   └── vite.svg
│   │   ├── src/
│   │   │   ├── assets/
│   │   │   │   └── react.svg
│   │   │   ├── components/
│   │   │   │   ├── Download.tsx
│   │   │   │   ├── Processing.tsx
│   │   │   │   ├── Submit.tsx
│   │   │   │   └── Upload.tsx
│   │   │   ├── App.css
│   │   │   ├── App.tsx
│   │   │   └── main.tsx
│   │   ├── .gitignore
│   │   ├── eslint.config.js
│   │   ├── index.html
│   │   ├── package-lock.json
│   │   ├── package.json
│   │   ├── README.md
│   │   ├── tsconfig.app.json
│   │   ├── tsconfig.json
│   │   ├── tsconfig.node.json
│   │   └── vite.config.ts
│   └── worker/
│       ├── src/
│       │   └── webp_worker/
│       │       ├── __init__.py
│       │       ├── __main__.py
│       │       ├── cli.py
│       │       ├── config.py
│       │       └── server.py
│       └── pyproject.toml
├── packages/
│   ├── converter/
│   │   ├── src/
│   │   │   └── webp_converter/
│   │   │       ├── __init__.py
│   │   │       ├── analysis.py
│   │   │       ├── convert.py
│   │   │       └── cwebp.py
│   │   └── pyproject.toml
│   └── shared/
│       ├── src/
│       │   └── webp_shared/
│       │       ├── __init__.py
│       │       ├── files.py
│       │       ├── protocol.py
│       │       ├── tcp.py
│       │       └── udp.py
│       └── pyproject.toml
├── scripts/
│   ├── setup/
│   │   ├── install_all.sh
│   │   ├── install_backend.sh
│   │   ├── install_converter.sh
│   │   ├── install_frontend.sh
│   │   ├── install_shared.sh
│   │   └── install_worker.sh
│   ├── run_backend.sh
│   ├── run_frontend.sh
│   └── run_worker.sh
├── tests/
│   ├── integration/
│   │   └── test_job_flow.py
│   ├── unit/
│   │   ├── test_analysis.py
│   │   ├── test_cwebp.py
│   │   └── test_protocol.py
│   └── __init__.py
├── .gitignore
├── pyproject.toml
└── README.md
```

## Files included

### Python files (.py) (contents included)

- apps/backend/pyproject.toml
- apps/backend/src/webp_backend/__init__.py
- apps/backend/src/webp_backend/app.py
- apps/backend/src/webp_backend/config.py
- apps/backend/src/webp_backend/routes/__init__.py
- apps/backend/src/webp_backend/routes/jobs.py
- apps/backend/src/webp_backend/routes/uploads.py
- apps/backend/src/webp_backend/services/__init__.py
- apps/backend/src/webp_backend/services/job_service.py
- apps/worker/pyproject.toml
- apps/worker/src/webp_worker/__init__.py
- apps/worker/src/webp_worker/__main__.py
- apps/worker/src/webp_worker/cli.py
- apps/worker/src/webp_worker/config.py
- apps/worker/src/webp_worker/server.py
- packages/converter/pyproject.toml
- packages/converter/src/webp_converter/__init__.py
- packages/converter/src/webp_converter/analysis.py
- packages/converter/src/webp_converter/convert.py
- packages/converter/src/webp_converter/cwebp.py
- packages/shared/pyproject.toml
- packages/shared/src/webp_shared/__init__.py
- packages/shared/src/webp_shared/files.py
- packages/shared/src/webp_shared/protocol.py
- packages/shared/src/webp_shared/tcp.py
- packages/shared/src/webp_shared/udp.py
- pyproject.toml
- scripts/run_backend.sh
- scripts/run_frontend.sh
- scripts/run_worker.sh
- scripts/setup/install_all.sh
- scripts/setup/install_backend.sh
- scripts/setup/install_converter.sh
- scripts/setup/install_frontend.sh
- scripts/setup/install_shared.sh
- scripts/setup/install_worker.sh
- tests/__init__.py
- tests/integration/test_job_flow.py
- tests/unit/test_analysis.py
- tests/unit/test_cwebp.py
- tests/unit/test_protocol.py

## Python file contents

### apps/backend/pyproject.toml

```python
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "webp-backend"
version = "0.1.0"
description = "Flask API and job orchestration for WebP conversion system"
requires-python = ">=3.10"
dependencies = [
    "flask>=3.0",
    "flask-cors>=4.0",
    "werkzeug>=3.0",
    "webp-shared",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
]

[project.scripts]
webp-backend = "webp_backend.app:main"

[tool.setuptools.packages.find]
where = ["src"]
include = ["webp_backend*"]
```

### apps/backend/src/webp_backend/__init__.py

```python
"""
WebP Backend - Flask API and job orchestration

This app is deployed on backend server. It:
1. Accepts uploads from frontend
2. Orchestrates jobs to workers over TCP
3. Receives results and serves them to clients

Deployment:
    pip install webp-shared webp-backend
    flask --app webp_backend.app:create_app run
"""

from .app import create_app
from .config import Config

__all__ = ["create_app", "Config"]
```

### apps/backend/src/webp_backend/app.py

```python
"""Flask application factory for the WebP backend."""

from __future__ import annotations

import logging
import sys

from flask import Flask
from flask_cors import CORS

from .config import Config
from .routes import jobs_bp, uploads_bp
from .services import JobService

logger = logging.getLogger(__name__)


def create_app(config: Config | None = None) -> Flask:
    """Create and configure the Flask application."""
    if config is None:
        config = Config.load()

    config.ensure_directories()

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    app = Flask(__name__)
    CORS(app, resources={r"/api/*": {"origins": "*"}})

    app.config["job_service"] = JobService(config)
    app.config["upload_dir"] = config.upload_dir
    app.config["extract_dir"] = config.extract_dir
    app.config["results_dir"] = config.results_dir

    app.register_blueprint(uploads_bp)
    app.register_blueprint(jobs_bp)

    @app.get("/health")
    def health():
        return {"status": "ok"}

    logger.info("WebP backend initialized")
    return app


def main() -> None:
    """Entry point for running the development server."""
    app = create_app()
    app.run(host="0.0.0.0", port=5001, debug=True, use_reloader=False)


if __name__ == "__main__":
    main()
```

### apps/backend/src/webp_backend/config.py

```python
"""Configuration management for the WebP backend."""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path


@dataclass(frozen=True)
class Config:
    """Backend configuration loaded from environment variables."""

    tcp_host: str = "127.0.0.1"
    tcp_port: int = 5055
    udp_port: int = 5056
    upload_dir: Path = Path("uploads")
    extract_dir: Path = Path("extracted")
    results_dir: Path = Path("results")
    heartbeat_timeout: float = 10.0

    @classmethod
    def load(cls) -> Config:
        """Load configuration from environment variables."""
        return cls(
            tcp_host=os.getenv("WEBP_TCP_HOST", "127.0.0.1"),
            tcp_port=int(os.getenv("WEBP_TCP_PORT", "5055")),
            udp_port=int(os.getenv("WEBP_UDP_PORT", "5056")),
            upload_dir=Path(os.getenv("WEBP_UPLOAD_DIR", "uploads")),
            extract_dir=Path(os.getenv("WEBP_EXTRACT_DIR", "extracted")),
            results_dir=Path(os.getenv("WEBP_RESULTS_DIR", "results")),
            heartbeat_timeout=float(os.getenv("WEBP_HEARTBEAT_TIMEOUT", "10.0")),
        )

    def ensure_directories(self) -> None:
        """Create all required directories."""
        self.upload_dir.mkdir(parents=True, exist_ok=True)
        self.extract_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
```

### apps/backend/src/webp_backend/routes/__init__.py

```python
"""Backend HTTP routes."""

from .jobs import jobs_bp
from .uploads import uploads_bp

__all__ = ["jobs_bp", "uploads_bp"]
```

### apps/backend/src/webp_backend/routes/jobs.py

```python
"""Job submission and polling routes."""

from __future__ import annotations

import logging
from pathlib import Path

from flask import Blueprint, abort, current_app, jsonify, request

from webp_shared.protocol import FileOptions, StartJob

logger = logging.getLogger(__name__)

jobs_bp = Blueprint("jobs", __name__, url_prefix="/api")


def _parse_int(value: str | None) -> int | None:
    if value is None or value == "":
        return None
    try:
        return int(value)
    except ValueError:
        return None


def _parse_str(value: str | None) -> str | None:
    return None if value is None or value == "" else value


@jobs_bp.post("/submit-job")
def submit_job():
    """Submit a single image for conversion."""
    job_service = current_app.config["job_service"]
    extract_dir: Path = current_app.config["extract_dir"]

    batch_id = _parse_int(request.form.get("batch_id"))
    job_id = _parse_int(request.form.get("image_id"))

    if batch_id is None or job_id is None:
        abort(400, description="batch_id and image_id are required")

    batch_dir = extract_dir / str(batch_id)
    input_file = None
    for ext in (".png", ".jpg", ".jpeg"):
        candidate = batch_dir / f"{job_id}{ext}"
        if candidate.exists():
            input_file = candidate
            break

    if input_file is None:
        abort(400, description=f"Image {job_id} not found in batch {batch_id}")

    options = FileOptions(
        width=_parse_int(request.form.get("width")),
        height=_parse_int(request.form.get("height")),
        size_type=_parse_str(request.form.get("size_type")) if not (
            _parse_int(request.form.get("width")) or _parse_int(request.form.get("height"))
        ) else None,
        crop_size_w=_parse_int(request.form.get("crop_size_w")),
        crop_size_h=_parse_int(request.form.get("crop_size_h")),
        crop_top_x=_parse_int(request.form.get("crop_top_x")),
        crop_top_y=_parse_int(request.form.get("crop_top_y")),
        crop_w=_parse_int(request.form.get("crop_w")),
        crop_h=_parse_int(request.form.get("crop_h")),
        lossless="lossless" in request.form,
        text_focus="text_focus" in request.form,
        has_text="has_text" in request.form,
        type=request.form.get("type", "default"),
    )

    job = {
        "job_id": job_id,
        "input_file": Path(input_file),
        "options": options,
    }

    if not job_service.start_job(job):
        return jsonify({"type": "no_worker"}), 503

    return "", 200


@jobs_bp.get("/get-next-job")
def get_next_job():
    """Poll for the next completed job result."""
    job_service = current_app.config["job_service"]

    if job_service.is_batch_complete():
        return jsonify({"type": "jobs_done"})

    result = job_service.get_next_result(timeout=3.0)

    if result is None:
        return jsonify({"type": "processing"})

    return jsonify(result)
```

### apps/backend/src/webp_backend/routes/uploads.py

```python
"""Upload and file serving routes."""

from __future__ import annotations

import json
import logging
from pathlib import Path

from flask import Blueprint, abort, current_app, jsonify, request, send_from_directory
from werkzeug.utils import secure_filename

from webp_shared.files import extract_files

logger = logging.getLogger(__name__)

uploads_bp = Blueprint("uploads", __name__, url_prefix="/api")


@uploads_bp.post("/upload-zip")
def upload_zip():
    """Upload a ZIP archive or single image for processing."""
    job_service = current_app.config["job_service"]
    extract_dir = current_app.config["extract_dir"]
    upload_dir = current_app.config["upload_dir"]

    batch_id = job_service.new_batch()

    f = request.files.get("file")
    if f is None:
        abort(400, description="Missing file field 'file'")

    filename = secure_filename(f.filename or "")
    if not filename:
        abort(400, description="Invalid filename")

    ext = Path(filename).suffix.lower()
    if ext not in {".png", ".jpg", ".jpeg", ".zip"}:
        abort(400, description="File must be .png, .jpg, .jpeg, or .zip")

    temp_path = upload_dir / f"{batch_id}{ext}"
    f.save(temp_path)

    dest_dir = extract_dir / str(batch_id)
    raw_images = extract_files(temp_path, dest_dir)

    if not raw_images:
        abort(400, description="No valid images found")

    manifest = {}
    images = []

    for job_id, raw_path_str in enumerate(raw_images):
        raw_path = Path(raw_path_str)
        new_name = f"{job_id}{raw_path.suffix}"
        new_path = dest_dir / new_name

        if raw_path != new_path:
            raw_path.rename(new_path)

        manifest[job_id] = {
            "original_name": raw_path.name,
            "original_ext": raw_path.suffix.lower(),
        }
        images.append({
            "job_id": job_id,
            "url": f"/api/files/{batch_id}/input/{job_id}",
            "original_name": raw_path.name,
        })

    (dest_dir / "manifest.json").write_text(json.dumps(manifest, indent=2))
    job_service.set_job_count(len(images))

    return jsonify({"batch_id": batch_id, "images": images})


@uploads_bp.get("/files/<int:batch_id>/input/<int:image_id>")
def serve_input_image(batch_id: int, image_id: int):
    """Serve an extracted input image for preview."""
    extract_dir: Path = current_app.config["extract_dir"]
    batch_dir = extract_dir / str(batch_id)

    if not batch_dir.exists():
        abort(404, description="Batch not found")

    for ext in (".png", ".jpg", ".jpeg"):
        candidate = batch_dir / f"{image_id}{ext}"
        if candidate.exists():
            mimetype = "image/jpeg" if ext == ".jpg" else f"image/{ext[1:]}"
            return send_from_directory(batch_dir, candidate.name, mimetype=mimetype)

    abort(404, description="Image not found")


@uploads_bp.get("/files/<int:batch_id>/output/<int:job_id>/<path:filename>")
def serve_output_webp(batch_id: int, job_id: int, filename: str):
    """Serve a converted WebP file."""
    results_dir: Path = current_app.config["results_dir"]

    safe_name = secure_filename(filename)
    if not safe_name or safe_name != filename:
        abort(400, description="Invalid filename")

    job_dir = results_dir / str(batch_id) / str(job_id)
    if not job_dir.exists():
        abort(404, description="Job not found")

    file_path = job_dir / safe_name
    if not file_path.exists():
        abort(404, description="File not found")

    return send_from_directory(job_dir, safe_name, mimetype="image/webp")
```

### apps/backend/src/webp_backend/services/__init__.py

```python
"""Backend services."""

from .job_service import JobService

__all__ = ["JobService"]
```

### apps/backend/src/webp_backend/services/job_service.py

```python
"""
Job Orchestration for the WebP backend.
"""
from __future__ import annotations

import logging
import threading
import time
from dataclasses import dataclass, field
from pathlib import Path
from queue import Empty, Queue
from typing import Any

from webp_shared.tcp import send_file_tcp, send_tcp, tcp_server
from webp_shared.udp import udp_server
from webp_shared import protocol

from ..config import Config

logger = logging.getLogger(__name__)

@dataclass
class WorkerState:
    """Tracks the state of a registered worker."""
    host: str
    port: int
    last_heartbeat: float = field(default_factory=time.time)
    status: str = "alive"
    active_jobs: list[int] = field(default_factory=list)


@dataclass
class JobState:
    """Tracks the state of a dispatched job."""
    job_id: int
    batch_id: int
    job_dict: dict[str, Any] = field(default_factory=dict)
    worker: int | None = None
    status: str = "pending"
    error: str | None = None


class JobService:
    """Manages job dispatch and worker coordination."""
    def __init__(self, config: Config):
        self._config = config
        self.results_dir = config.results_dir

        self._lock = threading.Lock()
        self._shutdown_event = threading.Event()

        self._workers: dict[int, WorkerState] = {}
        self._next_worker_id = 0

        self._batch_id = 0
        self._next_job_id = 0
        self._jobs: dict[int, JobState] = {}
        self._total_jobs = 0
        self._completed_jobs = 0
        self._results: Queue[dict[str, Any]] = Queue()

        self._tcp_thread = threading.Thread(
            target=tcp_server,
            args=(config.tcp_host, config.tcp_port, config.results_dir,
                  self._shutdown_event, self._handle_tcp_message),
            daemon=True,
        )
        self._tcp_thread.start()

        self._udp_thread = threading.Thread(
            target=udp_server,
            args=(config.tcp_host, config.udp_port,
                  self._shutdown_event, self._handle_heartbeat),
            daemon=True,
        )
        self._udp_thread.start()

        self._monitor_thread = threading.Thread(
            target=self._monitor_heartbeats,
            daemon=True,
        )
        self._monitor_thread.start()

        logger.info("JobService started on %s:%d", config.tcp_host, config.tcp_port)


    def shutdown(self) -> None:
        self._shutdown_event.set()
        with self._lock:
            workers = [
                (w.host, w.port) for w in self._workers.values()
            ]
        try:
            msg = protocol.Shutdown(
                host=self._config.tcp_host,
                port=self._config.tcp_port
            ).shutdown_dict()
        except protocol.ProtocolError as e:
            logger.error(e)
        for host, port in workers:
            try:
                send_tcp(host, port, msg)
            except Exception:
                pass
    

    def new_batch(self) -> int:
        """Start a new batch, clearing previous state."""
        with self._lock:
            self._batch_id += 1
            batch_id = self._batch_id
            self._jobs.clear()
            self._next_job_id = 0
            self._total_jobs = 0
            self._completed_jobs = 0
            self._results = Queue()
            workers = [
                (w.host, w.port) for w in self._workers.values()
            ]
            try:
                new_batch_msg = protocol.NewBatch(batch_id=self._batch_id).new_batch_dict()
            except protocol.ProtocolError as e:
                logger.error(e)
        for host, port in workers:
            try:
                send_tcp(host, port, new_batch_msg)
            except Exception as e:
                logger.warning("Failed to notify worker: %s", e)

        return batch_id
    
    def set_job_count(self, count: int) -> None:
        with self._lock:
            self._total_jobs = count

    def is_batch_complete(self) -> bool:
        with self._lock:
            return self._total_jobs > 0 and self._completed_jobs >= self._total_jobs

    def start_job(self, job_data: dict[str, Any]) -> bool:
        """Dispatch a job to an available worker."""
        with self._lock:
            job_id = job_data.get("job_id")
            if job_id is None:
                job_id = self._next_job_id
                self._next_job_id += 1
            job_data["job_id"] = job_id
            job_data["batch_id"] = self._batch_id

            alive = [(len(w.active_jobs), w_id) for w_id, w in self._workers.items()
                     if w.status == "alive"]
            if not alive:
                return False

            alive.sort(key=lambda x: x[0])
            _, id = alive[0]
            host = self._workers[id].host
            port = self._workers[id].port

            input_path = Path(job_data["input_file"])
            job_data["filename"] = f"job-{job_id}{input_path.suffix}"

            self._workers[id].active_jobs.append(job_id)
            self._jobs[job_id] = JobState(
                job_id=job_id,
                batch_id=self._batch_id,
                job_dict=dict(job_data),
                worker=(host, port),
                status="running",
            )

        try:
            file_data = protocol.SendFiles.start_job(
                job_data["batch_id"], job_data["job_id"],
                job_data["input_file"], job_data["filename"],
                job_data["options"]
            )
            send_file_tcp(host, port, file_data)
            return True
        except Exception as e:
            logger.error("Failed to dispatch job %d: %s", job_id, e)
            return False

    def get_next_result(self, timeout: float = 3.0) -> dict[str, Any] | None:
        """Get the next completed job result."""
        if self.is_batch_complete():
            return None
        try:
            result = self._results.get(timeout=timeout)
            with self._lock:
                self._completed_jobs += 1
            return result
        except Empty:
            return None

    def _handle_tcp_message(self, msg: dict[str, Any]) -> None:
        msg_type = msg.get("type")

        if msg_type == "shutdown":
            self._shutdown_event.set()
        elif msg_type == "new_convertor":
            self._register_worker(msg)
        elif msg_type == "images_ready":
            self._handle_job_complete(msg)
        elif msg_type == "job_error":
            self._handle_job_error(msg)

    def _register_worker(self, msg: dict[str, Any]) -> None:
        host = str(msg.get("host", ""))
        port = int(msg.get("port", 0))
        if not host or not port:
            return
        
        with self._lock:
            for w in self._workers.values():
                if w.host == host and w.port == port:
                    return
            
            key = self._next_worker_id
            self._next_worker_id += 1
            self._workers[key] = WorkerState(
                host=host,
                port=port,
            )

        logger.info("Registered worker %d at %s:%d", key, host, port)
        try:
            send_tcp(host, port, {"type": "ack", "id": key})
        except Exception as e:
            logger.error("Failed to ack worker: %s", e)

    def _handle_job_complete(self, msg: dict[str, Any]) -> None:
        batch_id = int(msg.get("batch_id", -1))
        job_id = int(msg.get("job_id", -1))
        worker_id = int(msg.get("worker_id", -1))

        with self._lock:
            if batch_id != self._batch_id:
                return
            if worker_id == -1 or worker_id not in self._workers:
                return
            worker = self._workers.get(worker_id)
            job = self._jobs.get(job_id)
            if job:
                job.status = "done"

            w_host, w_port = worker.host, worker.port
            if w_host and w_port:
                if worker and job_id in worker.active_jobs:
                    worker.active_jobs.remove(job_id)

        paths = msg.get("paths", [])
        urls = [f"/api/files/{batch_id}/output/{job_id}/{Path(p).name}" for p in paths]

        self._results.put({
            "type": "images",
            "batch_id": batch_id,
            "job_id": job_id,
            "urls": urls,
        })

    def _handle_job_error(self, msg: dict[str, Any]) -> None:
        job_id = int(msg.get("job_id", -1))
        batch_id = int(msg.get("batch_id", -1))

        with self._lock:
            if batch_id != self._batch_id:
                return
            job = self._jobs.get(job_id)
            if job:
                job.status = "error"
                job.error = msg.get("error")

        self._results.put({
            "type": "job_error",
            "batch_id": batch_id,
            "job_id": job_id,
            "error": msg.get("error"),
            "traceback": msg.get("traceback"),
        })

    def _handle_heartbeat(self, msg: dict[str, Any]) -> None:
        if msg.get("type") != "heartbeat":
            return
        worker_id = msg.get("worker_id")
        if not worker_id:
            return

        with self._lock:
            worker = self._workers.get(worker_id)
            if worker and worker.status == "alive":
                worker.last_heartbeat = msg["time"]

    def _monitor_heartbeats(self) -> None:
        """Detect dead workers and reassign their jobs."""
        while not self._shutdown_event.is_set():
            time.sleep(2.0)
            now = time.time()
            jobs_to_reassign = []

            with self._lock:
                for worker in self._workers.values():
                    if worker.status == "dead":
                        continue
                    if now - worker.last_heartbeat >= self._config.heartbeat_timeout:
                        logger.warning("Worker %s:%d is dead", worker.host, worker.port)
                        worker.status = "dead"
                        for job_id in worker.active_jobs:
                            job = self._jobs.get(job_id)
                            if job and job.status == "running":
                                jobs_to_reassign.append(job.job_dict)
                        worker.active_jobs.clear()

            for job_dict in jobs_to_reassign:
                self.start_job(job_dict)

```

### apps/worker/pyproject.toml

```python
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "webp-worker"
version = "0.1.0"
requires-python = ">=3.10"
dependencies = [
    "click>=8.1",
    "webp-shared",
    "webp-converter",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
]

[project.scripts]
webp-worker = "webp_worker.cli:cli"

[tool.setuptools.packages.find]
where = ["src"]
include = ["webp_worker*"]
```

### apps/worker/src/webp_worker/__init__.py

```python
"""
This app is deloyed on worker machines. It:
1. Registers with the backend
2. Receives conversion jobs over TCP
3. Runs the conversion (using webp-converter package)
4. Sends results back to the backend

Deployment:
    pip install webp-shared webp-converter webp-worker
    apt install webp  # for cwebp command
    webp-worker --backend-host <backend-ip>
"""

from .config import WorkerConfig
from .server import WorkerServer

__all__ = ["WorkerConfig", "WorkerServer"]
```

### apps/worker/src/webp_worker/__main__.py

```python
"""Allow running as python -m webp_worker."""

from .cli import main

if __name__ == "__main__":
    main()
```

### apps/worker/src/webp_worker/cli.py

```python
"""CLI for the WebP worker."""

from __future__ import annotations

import logging
import sys

import click

from webp_shared.files import find_free_tcp_port

from .config import WorkerConfig
from .server import WorkerServer


@click.command()
@click.option("-h", "--host", default="127.0.0.1", help="Worker host")
@click.option("-p", "--port", default=5057, type=int, help="Worker port")
@click.option("--backend-host", default="127.0.0.1", help="Backend host")
@click.option("--backend-port", default=5055, type=int, help="Backend TCP port")
@click.option("--backend-udp-port", default=5056, type=int, help="Backend UDP port")
@click.option("-v", "--verbose", is_flag=True, help="Debug logging")
def cli(host: str, port: int, backend_host: str, backend_port: int,
        backend_udp_port: int, verbose: bool) -> None:
    """Run a WebP conversion worker."""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    actual_port = find_free_tcp_port(host, port)
    if actual_port != port:
        logging.info("Port %d busy, using %d", port, actual_port)

    config = WorkerConfig(
        host=host,
        port=actual_port,
        backend_host=backend_host,
        backend_tcp_port=backend_port,
        backend_udp_port=backend_udp_port,
    )

    server = WorkerServer(config)
    try:
        server.run()
    except KeyboardInterrupt:
        logging.info("Interrupted")


def main() -> None:
    cli()


if __name__ == "__main__":
    main()
```

### apps/worker/src/webp_worker/config.py

```python
"""Configuration for the WebP worker."""

from __future__ import annotations

import os
import tempfile
from dataclasses import dataclass
from pathlib import Path


@dataclass(frozen=True)
class WorkerConfig:
    """Worker configuration."""

    host: str = "127.0.0.1"
    port: int = 5057
    backend_host: str = "127.0.0.1"
    backend_tcp_port: int = 5055
    backend_udp_port: int = 5056
    jobs_dir: Path = Path(tempfile.gettempdir()) / "webp-worker" / "jobs"
    output_dir: Path = Path(tempfile.gettempdir()) / "webp-worker" / "output"

    @classmethod
    def load(cls) -> WorkerConfig:
        """Load from environment variables."""
        return cls(
            host=os.getenv("WEBP_WORKER_HOST", "127.0.0.1"),
            port=int(os.getenv("WEBP_WORKER_PORT", "5057")),
            backend_host=os.getenv("WEBP_BACKEND_HOST", "127.0.0.1"),
            backend_tcp_port=int(os.getenv("WEBP_BACKEND_TCP_PORT", "5055")),
            backend_udp_port=int(os.getenv("WEBP_BACKEND_UDP_PORT", "5056")),
        )

    def ensure_directories(self) -> None:
        self.jobs_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)
```

### apps/worker/src/webp_worker/server.py

```python
"""Worker server implementation."""

from __future__ import annotations

import logging
import shutil
import threading
import traceback
from pathlib import Path
from queue import Empty, Queue
from typing import Any

from webp_shared.tcp import send_file_tcp, send_tcp, tcp_server
from webp_shared.udp import send_heartbeats
from webp_shared import protocol
from webp_converter import ConversionJob

from .config import WorkerConfig

logger = logging.getLogger(__name__)


class WorkerServer:
    """Worker process that receives and executes conversion jobs."""

    def __init__(self, config: WorkerConfig):
        self._config = config
        config.ensure_directories()

        self._host = config.host
        self._port = config.port
        self._backend_host = config.backend_host
        self._backend_tcp_port = config.backend_tcp_port
        self._backend_udp_port = config.backend_udp_port
        self._jobs_dir = config.jobs_dir
        self._output_dir = config.output_dir

        self._lock = threading.Lock()
        self._batch_id = -1
        self._worker_id: int | None = None

        self._shutdown = threading.Event()
        self._new_batch = threading.Event()
        self._registered = threading.Event()
        self._job_queue: Queue[dict[str, Any]] = Queue()

    def run(self) -> None:
        """Run the worker until shutdown."""
        logger.info("Starting worker at %s:%d", self._host, self._port)

        tcp_thread = threading.Thread(
            target=tcp_server,
            args=(self._host, self._port, self._jobs_dir,
                  self._shutdown, self._handle_message),
            daemon=True,
        )
        tcp_thread.start()

        if not self._register_with_backend():
            logger.error("Failed to register")
            self._shutdown.set()
            return
        

        heartbeat_thread = threading.Thread(
            target=send_heartbeats,
            args=(self._worker_id, self._backend_host,
                  self._backend_udp_port, self._shutdown),
            daemon=True,
        )
        heartbeat_thread.start()

        self._process_jobs()

        logger.info("Worker stopped")

    def _register_with_backend(self, timeout: float = 30.0) -> bool:
        """Register with backend and wait for ack."""
        for attempt in range(4):
            try:
                register_msg = protocol.WorkerRegistration(
                    host=self._host,
                    port=self._port,
                ).get_reg_dict()
                send_tcp(self._backend_host, 
                         self._backend_tcp_port, 
                         register_msg
                )
            except Exception as e:
                logger.warning("Registration attempt %d failed: %s", attempt, e)
                continue

            if self._registered.wait(timeout=10.0):
                logger.info("Registered as worker %d", self._worker_id)
                return True

        return False

    def _process_jobs(self) -> None:
        """Main job processing loop."""
        while not self._shutdown.is_set():
            try:
                msg = self._job_queue.get(timeout=1.0)
            except Empty:
                continue

            if msg.get("type") == "new_batch":
                self._handle_new_batch(msg)
                continue

            if msg.get("type") != "new_job":
                continue

            if msg.get("batch_id") != self._batch_id:
                continue

            self._process_single_job(msg)

    def _process_single_job(self, msg: dict[str, Any]) -> None:
        """Process a single conversion job."""
        job_id = int(msg.get("job_id", -1))
        batch_id = self._batch_id

        logger.info("Processing job %d", job_id)

        out_dir = self._output_dir / str(job_id)
        out_dir.mkdir(parents=True, exist_ok=True)

        try:
            options_dict = msg.get("options", {})
            file_options = protocol.parse_file_options(options_dict)

            worker_job = protocol.WorkerJob(
                input_file=msg.get("saved_path"),
                out_path=str(out_dir),
                job_id=job_id,
                options=file_options,
            )

            conversion = ConversionJob(
                worker_job,
                new_batch_event=self._new_batch,
                shutdown_event=self._shutdown,
            )
            result = conversion.run()

            if self._new_batch.is_set() or self._shutdown.is_set():
                return

            zip_path = self._output_dir / f"result-{self._worker_id}-{job_id}.zip"
            shutil.make_archive(str(zip_path.with_suffix("")), "zip", root_dir=str(out_dir))

            images_ready = protocol.SendFiles.img_ready_msg(
                batch_id, job_id, self._worker_id, zip_path
            )

            send_file_tcp(self._backend_host, self._backend_tcp_port, images_ready)

            logger.info("Job %d complete: %d files", job_id, len(result.output_files))

        except Exception as e:
            error_msg = f"{type(e).__name__}: {e}"
            logger.error("Job %d failed: %s", job_id, error_msg)

            try:
                error_msg = protocol.JobError(
                    batch_id=batch_id,
                    job_id=job_id,
                    w_id=self._worker_id,
                    error=error_msg,
                    traceback=traceback.format_exc(),
                ).error_dict()

                send_tcp(self._backend_host, self._backend_tcp_port, error_msg)
            except protocol.ProtocolError as e:
                logger.error(f"Protocol error {e}")
            
            except Exception:
                pass

    def _handle_new_batch(self, msg: dict[str, Any]) -> None:
        """Handle new batch notification."""
        with self._lock:
            self._batch_id = int(msg.get("batch_id", -1))

        self._new_batch.set()

        if self._output_dir.exists():
            shutil.rmtree(self._output_dir)
        self._output_dir.mkdir(parents=True, exist_ok=True)

        while not self._job_queue.empty():
            try:
                self._job_queue.get_nowait()
            except Empty:
                break

        self._new_batch.clear()
        logger.info("Switched to batch %d", self._batch_id)

    def _handle_message(self, msg: dict[str, Any]) -> None:
        """Handle TCP message."""
        msg_type = msg.get("type")

        if msg_type == "shutdown":
            self._shutdown.set()
        elif msg_type == "ack":
            self._worker_id = msg.get("id")
            self._registered.set()
        elif msg_type == "new_batch":
            with self._lock:
                self._batch_id = int(msg.get("batch_id", -1))
            self._job_queue.put(msg)
        elif msg_type == "new_job":
            if msg.get("batch_id") == self._batch_id:
                self._job_queue.put(msg)
```

### packages/converter/pyproject.toml

```python
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "webp-converter"
version = "0.1.0"
description = "Core WebP conversion logic with cwebp wrapper and image analysis"
requires-python = ">=3.10"
dependencies = [
    "pillow>=10.0",
    "numpy>=1.24",
    "opencv-python>=4.8",
    "pillow-heif>=0.15",
    "webp-shared",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
]

[tool.setuptools.packages.find]
where = ["src"]
include = ["webp_converter*"]
```

### packages/converter/src/webp_converter/__init__.py

```python
"""
WebP Conversion Engine.

This package is the core image-webp conversion logic.
It is used only by workers.

Deployment:
    pip install webp-convertor

This package has no networking dependencies. It's pure image processing.

"""

from .analysis import analyze_image, center_background_contrast, edge_density
from .convert import ConversionJob, ConversionResult
from .cwebp import CwebpError, convert_with_retry, run_cwebp

__all__ = [
    "edge_density",
    "center_background_contrast",
    "analyze_image",
    "CwebpError",
    "run_cwebp",
    "convert_with_retry",
    "ConversionJob",
    "ConversionResult",
]
```

### packages/converter/src/webp_converter/analysis.py

```python
"""
Image analysis utilities for quality optimization.

Analyzes images to provide factors that can strengthen
hardcoded parameters decided by user input.
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Tuple

import cv2
import numpy as np
from PIL import Image, ImageOps

logger = logging.getLogger(__name__)

def edge_density(gray: np.ndarray) -> float:
    """
    Calculate the edge density of a grayscale image.

    Returns Edge density in range [0.0, 1.0]
    """
    blurred = cv2.GaussianBlur(gray, (3, 3), 0)
    edges = cv2.Canny(blurred, threshold1=100, threshold2=200, L2gradient=True)
    edge_pixels = np.count_nonzero(edges)
    total_pixels = edges.size
    return edge_pixels / total_pixels if total_pixels > 0 else 0.0


def center_background_contrast(gray: np.ndarray) -> float:
    """
    Calculate contrast between image center and edges.

    Returns: Absolute difference in mean brightness [0.0, 255.0]
    """
    h, w = gray.shape
    cx0, cx1 = int(w * 0.30), int(w * 0.70)
    cy0, cy1 = int(h * 0.30), int(h * 0.70)

    center = gray[cy0:cy1, cx0:cx1]
    border = gray.copy()
    border[cy0:cy1, cx0:cx1] = 0
    border_vals = border[border > 0]

    if border_vals.size == 0:
        return 0.0

    return float(abs(center.mean() - border_vals.mean()))


def analyze_image(image_path: Path) -> Tuple[float, float]:
    """
    Analyze an image and return quality hints.

    Returns Tuple of (edge_density, center_contrast)

    Raises ValueError: If image is multi-frame
    """
    img = Image.open(image_path)

    n_frames = getattr(img, "n_frames", 1)
    if n_frames != 1:
        raise ValueError(f"Multi-frame image not supported: {image_path}")

    img = ImageOps.exif_transpose(img)

    has_alpha = img.mode in ("RGBA", "LA") or (
        img.mode == "P" and "transparency" in img.info
    )

    if has_alpha:
        img = img.convert("RGBA")
        rgba = np.array(img, dtype=np.uint8)
        bgr = cv2.cvtColor(rgba, cv2.COLOR_RGBA2BGR)
    else:
        img = img.convert("RGB")
        rgb = np.array(img, dtype=np.uint8)
        bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)

    bgr = np.ascontiguousarray(bgr)

    h, w = bgr.shape[:2]
    max_dim = max(h, w)
    if max_dim > 512:
        scale = 512 / max_dim
        bgr = cv2.resize(bgr, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)

    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)

    ed = edge_density(gray)
    cc = center_background_contrast(gray)

    logger.debug("Analysis for %s: edge_density=%.3f, contrast=%.1f", image_path.name, ed, cc)
    return ed, cc
```

### packages/converter/src/webp_converter/convert.py

```python
"""
Image conversion orchestration for WebP output.

This module handles the high-level conversion workflow:
1. Pre-process (crop, resize large images)
2. Choose quality/size variants based on image analysis
3. Run cwebp multiple times with different settings
"""

from __future__ import annotations

import logging
import threading
from dataclasses import dataclass, field
from pathlib import Path

from PIL import Image, ImageOps

# Note: These imports come from webp_shared, which must be installed
# This is a peer dependency - the worker installs both packages
from webp_shared.protocol import FileOptions, WorkerJob

from .analysis import analyze_image
from .cwebp import CwebpError, convert_with_retry

logger = logging.getLogger(__name__)

SIZE_PRESETS: dict[str, list[int]] = {
    "banner": [1200, 1500, 1800],
    "content": [800, 1000, 1200, 1400],
    "thumbnail": [400, 500, 650, 800],
    "icon": [96, 128, 256],
    "default": [600, 800, 1000, 1200],
}

MAX_DIMENSION = 8000
MAX_PIXELS = 50_000_000


@dataclass
class ConversionResult:
    """Result of a conversion job."""
    output_files: list[Path] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)


class ConversionJob:
    """
    Orchestrates the conversion of a single image to WebP.

    The job generates multiple output variants at different sizes and
    quality levels based on the FileOptions.
    """

    def __init__(
        self,
        job: WorkerJob,
        new_batch_event: threading.Event | None = None,
        shutdown_event: threading.Event | None = None,
    ):
        self._new_batch = new_batch_event or threading.Event()
        self._shutdown = shutdown_event or threading.Event()

        if job.input_file is None:
            raise ValueError("WorkerJob.input_file is required")
        if job.out_path is None:
            raise ValueError("WorkerJob.out_path is required")

        self.input_file = Path(job.input_file)
        self.output_dir = Path(job.out_path)
        self.options: FileOptions = job.options or FileOptions()

        self._working_file: Path = self.input_file
        self._size_args: list[list[str]] = []
        self.quality_factor: float = 1.0
        self.sharpness: int = 4

        if self.options.has_text:
            self.sharpness = 1

        ext = self.input_file.suffix.lower()
        if ext not in (".jpg", ".jpeg", ".png"):
            raise ValueError(f"Unsupported input format: {ext}")

    def should_stop(self) -> bool:
        """Check if the job should be cancelled."""
        return self._new_batch.is_set() or self._shutdown.is_set()

    def run(self) -> ConversionResult:
        """Execute the conversion job."""
        result = ConversionResult()

        if self.should_stop():
            return result

        self._preprocess()
        if self.should_stop():
            return result

        self._choose_sizes()
        if self.should_stop():
            return result

        self.output_dir.mkdir(parents=True, exist_ok=True)
        variants = self._get_quality_variants()

        if not self._size_args:
            self._size_args = [[]]

        size0 = self._size_args[0]
        size1 = self._size_args[1] if len(self._size_args) >= 2 else size0

        for i in range(12):
            if self.should_stop():
                break

            size_args = size0 if i < 6 else size1
            variant_args = variants[i % 6]
            output_path = self.output_dir / f"{i}.webp"

            try:
                convert_with_retry(
                    self._working_file,
                    output_path,
                    size_args + variant_args,
                    max_retries=4,
                )
                if output_path.exists():
                    result.output_files.append(output_path)
            except CwebpError as e:
                result.errors.append(f"Variant {i}: {e}")
                logger.warning("Variant %d failed: %s", i, e)
            except Exception as e:
                result.errors.append(f"Variant {i}: {type(e).__name__}: {e}")

        if not result.output_files:
            raise RuntimeError(f"All variants failed. Errors: {result.errors}")

        logger.info("Conversion complete: %d files", len(result.output_files))
        return result

    def _preprocess(self) -> None:
        """Apply cropping and handle oversized images."""
        with Image.open(self.input_file) as img:
            img = ImageOps.exif_transpose(img)
            width, height = img.size
            modified = False

            if self.options.has_crop():
                scale_x = width / self.options.crop_size_w
                scale_y = height / self.options.crop_size_h

                x = int(round(self.options.crop_top_x * scale_x))
                y = int(round(self.options.crop_top_y * scale_y))
                w = int(round(self.options.crop_w * scale_x))
                h = int(round(self.options.crop_h * scale_y))

                img = img.crop((x, y, x + w, y + h))
                width, height = w, h
                modified = True

            if max(width, height) >= MAX_DIMENSION or (width * height) >= MAX_PIXELS:
                while max(width, height) >= MAX_DIMENSION or (width * height) >= MAX_PIXELS:
                    width = int(width * 0.95)
                    height = int(height * 0.95)
                img = img.resize((width, height), Image.Resampling.LANCZOS)
                modified = True

            if modified:
                processed_path = self.input_file.with_name(
                    f"{self.input_file.stem}_processed{self.input_file.suffix}"
                )
                img.save(processed_path)
                self._working_file = processed_path

    def _choose_sizes(self) -> None:
        """Determine output sizes based on options and analysis."""
        if self.options.has_explicit_size():
            w = self.options.width or 0
            h = self.options.height or 0
            self._size_args = [["-resize", str(w), str(h)]]
            return

        size_key = self.options.size_type or "default"
        if size_key not in SIZE_PRESETS:
            size_key = "default"
        widths = SIZE_PRESETS[size_key]

        if self.options.type in ("product", "complex", "default"):
            ed, cc = analyze_image(self._working_file)

            if ed <= 0.05 and cc < 15:
                chosen = widths[:2]
                self.quality_factor = 1.06
            elif ed <= 0.08 and cc < 20:
                chosen = widths[1:3] if len(widths) >= 3 else widths[:2]
                self.quality_factor = 1.03
            else:
                chosen = widths[2:4] if len(widths) >= 4 else widths[-2:]

            if len(chosen) < 2:
                chosen = (widths + widths)[:2]

            self._size_args = [
                ["-resize", str(chosen[0]), "0"],
                ["-resize", str(chosen[1]), "0"],
            ]
            return

        if self.options.type == "graphic":
            self._size_args = [["-resize", "150", "0"], ["-resize", "250", "0"]]
            return

        self._size_args = [[]]

    def _get_quality_variants(self) -> list[list[str]]:
        """Get cwebp argument sets for quality variants."""
        q = lambda x: str(int(round(x * self.quality_factor)))

        if self.options.lossless:
            return [
                ["-lossless", "-m", "6", "-q", "100"],
                ["-lossless", "-z", "9"],
                ["-lossless", "-z", "6"],
                ["-near_lossless", "80"],
                ["-near_lossless", "60"],
                ["-near_lossless", "40"],
            ]

        if self.options.text_focus:
            return [
                ["-preset", "text", "-lossless", "-m", "6", "-q", "100", "-alpha_q", "100", "-exact"],
                ["-preset", "text", "-lossless", "-z", "6", "-alpha_q", "100", "-exact"],
                ["-preset", "text", "-lossless", "-z", "9", "-alpha_q", "100", "-exact"],
                ["-preset", "text", "-near_lossless", "90", "-m", "6", "-alpha_q", "100"],
                ["-preset", "text", "-near_lossless", "70", "-m", "6", "-alpha_q", "100"],
                ["-preset", "text", "-m", "6", "-q", "94", "-alpha_q", "100", "-alpha_filter", "best"],
            ]

        t = self.options.type

        if t == "product":
            s = str(self.sharpness)
            return [
                ["-m", "6", "-q", q(92), "-af", "-sharpness", s],
                ["-preset", "picture", "-m", "6", "-q", q(88), "-af"],
                ["-preset", "picture", "-m", "6", "-q", q(86), "-af"],
                ["-preset", "picture", "-m", "6", "-q", q(84), "-af"],
                ["-preset", "picture", "-m", "6", "-q", q(82), "-af"],
                ["-preset", "picture", "-m", "6", "-q", q(80), "-af"],
            ]

        if t == "complex":
            return [
                ["-preset", "photo", "-m", "6", "-q", "96", "-af"],
                ["-preset", "photo", "-m", "6", "-q", "94", "-af"],
                ["-preset", "photo", "-m", "6", "-q", "92", "-af"],
                ["-preset", "photo", "-m", "6", "-q", "90", "-af"],
                ["-preset", "photo", "-m", "6", "-q", "85", "-af"],
                ["-preset", "photo", "-m", "6", "-q", "82", "-af"],
            ]

        if t == "graphic":
            return [
                ["-preset", "drawing", "-lossless", "-m", "6", "-q", "100", "-alpha_q", "100", "-alpha_filter", "best", "-exact"],
                ["-preset", "drawing", "-lossless", "-z", "9", "-alpha_q", "100", "-alpha_filter", "best", "-exact"],
                ["-preset", "drawing", "-lossless", "-z", "6", "-alpha_q", "100", "-alpha_filter", "best", "-exact"],
                ["-preset", "drawing", "-near_lossless", "90", "-m", "6", "-alpha_q", "100"],
                ["-preset", "drawing", "-near_lossless", "75", "-m", "6", "-alpha_q", "100"],
                ["-preset", "drawing", "-near_lossless", "60", "-m", "6", "-alpha_q", "100"],
            ]

        return [
            ["-lossless", "-m", "6", "-q", "100"],
            ["-preset", "photo", "-m", "6", "-q", "96", "-af"],
            ["-preset", "picture", "-m", "6", "-q", q(90), "-af"],
            ["-preset", "photo", "-m", "6", "-q", q(90)],
            ["-m", "6", "-q", q(90), "-sns", "30", "-af"],
            ["-m", "6", "-q", q(90), "-sns", "40"],
        ]
```

### packages/converter/src/webp_converter/cwebp.py

```python
"""
Wrapper for the cwebp command-line tool.

This module provides a Python interface to cwebp with:
- Proper error handling and custom exceptions
- Automatic retry on partition overflow errors
- Image resizing on retry to avoid memory issues
"""

from __future__ import annotations

import logging
import subprocess
from pathlib import Path

from PIL import Image

logger = logging.getLogger(__name__)

DEFAULT_TIMEOUT = 120.0


class CwebpError(RuntimeError):
    """Raised when cwebp fails to convert an image."""

    def __init__(self, command: list[str], returncode: int, stderr: str):
        self.command = command
        self.returncode = returncode
        self.stderr = stderr
        super().__init__(f"cwebp failed (rc={returncode}): {stderr.strip()}")


def run_cwebp(args: list[str], timeout: float = DEFAULT_TIMEOUT) -> tuple[int, str, str]:
    """Run cwebp with the given arguments."""
    try:
        result = subprocess.run(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            check=False,
            timeout=timeout,
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return 124, "", f"TimeoutExpired after {timeout}s"
    except FileNotFoundError:
        return 127, "", "cwebp not found. Install webp package."


def _is_partition_overflow(stderr: str) -> bool:
    """Check if error is a partition overflow (retry-able)."""
    if not stderr:
        return False
    return "PARTITION0_OVERFLOW" in stderr or "Error code: 6" in stderr


def _is_timeout(returncode: int) -> bool:
    """Check if the error was a timeout."""
    return returncode == 124


def _shrink_resize_args(cmd: list[str], scale: float, input_path: Path) -> list[str]:
    """Add or adjust -resize arguments to shrink the output."""
    cmd = list(cmd)

    if "-resize" in cmd:
        idx = cmd.index("-resize")
        width = int(cmd[idx + 1])
        height = int(cmd[idx + 2])

        if width > 0:
            width = max(1, int(width * scale))
        if height > 0:
            height = max(1, int(height * scale))

        cmd[idx + 1] = str(width)
        cmd[idx + 2] = str(height)
    else:
        with Image.open(input_path) as img:
            width, height = img.size
        width = max(1, int(width * scale))
        height = max(1, int(height * scale))
        cmd[2:2] = ["-resize", str(width), str(height)]

    return cmd


def convert_with_retry(
    input_path: Path,
    output_path: Path,
    cwebp_args: list[str],
    max_retries: int = 4,
    timeout: float = DEFAULT_TIMEOUT,
) -> None:
    """
    Convert an image to WebP, retrying with shrinking on overflow.

    Raises:
        CwebpError: If all attempts fail
        FileNotFoundError: If input file doesn't exist
    """
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    cmd = ["cwebp", str(input_path), "-o", str(output_path), "-mt"] + cwebp_args

    logger.debug("Running: %s", " ".join(cmd))
    returncode, stdout, stderr = run_cwebp(cmd, timeout)

    if returncode == 0:
        return

    if not _is_partition_overflow(stderr) and not _is_timeout(returncode):
        raise CwebpError(cmd, returncode, stderr)

    retry_cmd = list(cmd)
    for attempt in range(1, max_retries + 1):
        scale = 1.0 - (attempt * 0.1)
        retry_cmd = _shrink_resize_args(retry_cmd, scale, input_path)

        logger.info("Retry %d/%d with scale %.1f", attempt, max_retries, scale)
        returncode, stdout, stderr = run_cwebp(retry_cmd, timeout)

        if returncode == 0:
            return

        if not _is_partition_overflow(stderr) and not _is_timeout(returncode):
            raise CwebpError(retry_cmd, returncode, stderr)

    raise CwebpError(retry_cmd, returncode, stderr)
```

### packages/shared/pyproject.toml

```python
[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "webp-shared"
version = "0.1.0"
description = "Shared protocol and networking for WebP conversion system"
requires-python = ">=3.10"
dependencies = [
    "werkzeug>=3.0",  # For secure_filename
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
]

[tool.setuptools.packages.find]
where = ["src"]
include = ["webp_shared*"]
```

### packages/shared/src/webp_shared/__init__.py

```python
"""
Shared networking and protocol types for WebP conversion

The package is a dependency of both backend and worker:
- Backend uses it for TCP/UDP servers and protocol types
- Worker uses it for TCP/UDP clients and protocol types

Deployment:
    pip install webp-shared
"""

from .protocol import (
    PROTOCOL_VERSION,
    FileOptions,
    Heartbeat,
    SendFiles,
    StartJob,
    ImagesReadyHeader,
    JobError,
    NewBatch,
    ProtocolError,
    Shutdown,
    WorkerAck,
    WorkerJob,
    WorkerRegistration,
    parse_file_options,
    validate_version,
)
from .tcp import (
    ConnectionFailed,
    RecvFailed,
    SendFailed,
    TCPError,
    send_file_tcp,
    send_tcp,
    tcp_server,
)
from .udp import (
    send_heartbeats,
    send_udp,
    udp_server,
)
from .files import (
    ALLOWED_IMG_EXTS,
    extract_files,
    find_free_tcp_port,
    is_in_dir,
)

__all__ = [
    # Protocol
    "PROTOCOL_VERSION",
    "ProtocolError",
    "validate_version",
    "FileOptions",
    "SendFiles",
    "StartJob",
    "ImagesReadyHeader",
    "WorkerJob",
    "JobError",
    "Heartbeat",
    "WorkerRegistration",
    "WorkerAck",
    "NewBatch",
    "Shutdown",
    "parse_file_options",
    # TCP
    "TCPError",
    "ConnectionFailed",
    "SendFailed",
    "RecvFailed",
    "send_tcp",
    "send_file_tcp",
    "tcp_server",
    # UDP
    "send_udp",
    "send_heartbeats",
    "udp_server",
    # Files
    "ALLOWED_IMG_EXTS",
    "is_in_dir",
    "extract_files",
    "find_free_tcp_port",
]
```

### packages/shared/src/webp_shared/files.py

```python
"""
File handling utilities for app and Worker
"""

from __future__ import annotations

import errno
import logging
import shutil
import socket
import zipfile
from pathlib import Path

from werkzeug.utils import secure_filename

logger = logging.getLogger(__name__)

ALLOWED_IMG_EXTS: frozenset[str] = frozenset({".png", ".jpg", ".jpeg"})


def is_in_dir(base: Path, target: Path) -> bool:
    """Check if target path is in base dir."""
    try:
        target.resolve().relative_to(base.resolve())
        return True
    except ValueError:
        return False


def extract_files(file_path: Path, dest_dir: Path) -> list[str]:
    dest_dir.mkdir(parents=True, exist_ok=True)
    suffix = file_path.suffix.lower()

    if suffix in ALLOWED_IMG_EXTS:
        safe_name = secure_filename(file_path.name)
        if not safe_name:
            logger.warning("Invalid filename: %s", file_path.name)
            return []
        
        out_path = dest_dir / safe_name
        if not is_in_dir(dest_dir, out_path):
            logger.warning("Path traversal attempt: %s", file_path.name)
            return []
        
        try:
            shutil.copyfile(file_path, out_path)
            return [str(out_path)]
        except OSError as e:
            logger.error("Failed to copy %s: %s", file_path, e)
            return []
    
    if suffix != ".zip":
        logger.warning("Unsupported file type: %s", suffix)
        return []
    
    extracted_files: list[str] = []
    
    try:
        with zipfile.ZipFile(file_path) as z:
            for info in z.infolist():
                if info.is_dir():
                    continue

                raw_name = info.filename

                if raw_name.startswith("__MACOSX/") or raw_name.endswith(".DS_Store"):
                    continue

                ext = Path(raw_name).suffix.lower()
                if ext not in ALLOWED_IMG_EXTS:
                    continue

                safe_name = secure_filename(Path(raw_name).name)

                if not safe_name:
                    continue

                out_path = dest_dir / safe_name

                if not is_in_dir(dest_dir, out_path):
                    continue
                
                try:
                    with z.open(info) as src, open(out_path, "wb") as dst:
                        dst.write(src.read())
                    extracted_files.append(str(out_path))
                except (OSError, zipfile.BadZipFile) as e:
                    logger.error("Failed to extract %s: %s", raw_name, e)
    except zipfile.BadZipFile as e:
        logger.error("Invalid ZIP file %s: %s", file_path, e)
        return []
    
    logger.info("Extracted %d files from %s", len(extracted_files), file_path.name)
    return extracted_files


def find_free_tcp_port(host: str, start_port: int, max_tries: int = 100) -> int:
    """Find an available TCP port starting from start_port."""
    if not (0 <= start_port <= 65535):
        raise ValueError(f"Port must be 0..65535, got {start_port}")

    for port in range(start_port, min(65536, start_port + max_tries)):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.bind((host, port))
            logger.debug("Found free port: %d", port)
            return port
        except OSError as e:
            if e.errno in (errno.EADDRINUSE, errno.EACCES):
                continue
            raise
        finally:
            sock.close()

    raise RuntimeError(f"No free TCP port found in range {start_port}-{start_port + max_tries}")
```

### packages/shared/src/webp_shared/protocol.py

```python
"""
Protocol definitions for CwebP conversion pipeline.

Message Flow:
    Worker -> Backend: type = new_convertor (register worker)
    Backend -> Worker: type = ack (Tell worker it's registered)
    Worker -> Backend: type = heartbeat (UDP, every 1-2s)
    Backend -> Worker: type = new_batch (clears Worker's state)
    Backend -> Worker: type = new_job (A job is one file)
    Worker -> Backend: type = images_ready (sends a zip of converted versions)
    Either -> Either: type = shutdown (terminate all. Only allow Worker -> Backend for simplicity and ease of testing)
"""
from __future__ import annotations

import time
import json
import struct
from dataclasses import dataclass, asdict
from typing import Literal, Any
from pathlib import Path

PROTOCOL_VERSION: Literal[1] = 1

# Type literal
SizeType = Literal["banner", "content", "thumbnail", "icon", "other"]
ImageType = Literal["complex", "graphic", "product", "default"]
JobStage = Literal["convert", "zip", "unknown"]
JobState = Literal["queued", "running", "done", "error"]


class ProtocolError(Exception):
    """Raised when a message fails validation or version check."""
    pass


def validate_version(msg: dict[str, Any]) -> None:
    version = msg.get("v")
    if version is not None and version != PROTOCOL_VERSION:
        raise ProtocolError(
            f"Protocol version mismatch: expected {PROTOCOL_VERSION}."
        )


@dataclass
class FileOptions:
    """
    Conversion options for an image. Each of these options can
    be speciefied on the frontend.
    """
    lossless: bool = False
    text_focus: bool = False
    has_text: bool = False

    type: ImageType = "default"

    crop_size_w: int | None = None
    crop_size_h: int | None = None
    crop_top_x: int | None = None
    crop_top_y: int | None = None
    crop_w: int | None = None
    crop_h: int | None = None

    size_type: SizeType = "content"
    width: int | None = None
    height: int | None = None

    def has_crop(self) -> bool:
        """True if all crop parameters are set."""
        return all(
            v is not None
            for v in [
                self.crop_size_w, self.crop_size_h,
                self.crop_top_x, self.crop_top_y,
                self.crop_w, self.crop_h,
            ]
        )
    

    def has_explicit_size(self) -> bool:
        """True if explicit width or height is set."""
        return self.width is not None or self.height is not None

@dataclass
class SendFiles:
    """For sending new_job to workers or images_ready to app."""
    prefix: bytes | None = None
    header: bytes | None = None
    file_bytes: bytes | None = None

    @classmethod
    def start_job(
        cls,
        batch_id: int,
        job_id: int,
        input_file: Path,
        filename: str,
        options: FileOptions,
    ) -> "SendFiles":
        if not input_file.exists():
            raise ProtocolError(
                "Input file doesn't exist for new_job."
            )
        img_bytes = input_file.read_bytes()
        byte_len = len(img_bytes)
        filename = input_file.name
        header_bytes = StartJob.from_submitted(
            batch_id, job_id, filename, options, byte_len
        ).get_bytes()
        prefix = struct.pack(">I", len(header_bytes))

        return cls(
            prefix=prefix,
            header=header_bytes,
            file_bytes=img_bytes,
        )
    

    @classmethod
    def img_ready_msg(
        cls,
        batch_id: int,
        job_id: int,
        w_id: int,
        in_file: Path,
    ) -> "SendFiles":
        if not in_file.exists():
            raise FileNotFoundError(f"Input file not found: {in_file}")

        img_bytes = in_file.read_bytes()
        byte_len = len(img_bytes)
        filename = in_file.name
        header_bytes = ImagesReadyHeader.make_header(
            batch_id, job_id, w_id, filename, byte_len
        ).get_bytes()
        prefix = struct.pack(">I", len(header_bytes))

        return cls(
            prefix=prefix,
            header=header_bytes,
            file_bytes=img_bytes,
        )


@dataclass
class StartJob:
    """Backend -> Worker to start new conversion job."""
    v: int = PROTOCOL_VERSION
    type: str = "new_job"
    batch_id: int | None = None
    job_id: int | None = None
    filename: str | None = None
    options: FileOptions | None = None
    byte_length: int | None = None


    @classmethod
    def from_submitted(
        cls,
        batch_id: int,
        job_id: int,
        filename: str,
        options: FileOptions,
        bytelength: int
    ) -> "StartJob":
        """Create from a submitted job."""
        return cls(
            batch_id=batch_id,
            job_id=job_id,
            filename=filename,
            options=options,
            byte_length=bytelength,
        )
    def get_bytes(self) -> bytes:
        return json.dumps(self.to_dict()).encode("utf-8")

    def to_dict(self) -> dict[str, Any]:
        d = asdict(self)
        if self.options is not None:
            d["options"] = asdict(self.options)
        return d


@dataclass
class ImagesReadyHeader:
    """Class for Images Ready Header."""
    v: int = PROTOCOL_VERSION
    type: Literal["images_ready"] = "images_ready"
    batch_id: int | None = None
    job_id: int | None = None
    worker_id: int | None = None
    format: Literal["zip"] = "zip"
    filename: str | None = None
    content_type: str = "application/zip"
    byte_length: int | None = None

    @classmethod
    def make_header(
        cls,
        batch_id: int,
        job_id: int,
        worker_id: int,
        filename: str,
        byte_length: int
    ) -> "ImagesReadyHeader":
        """Make ImagesReadyHeader, called in ImagesReady class method."""
        return cls(
            batch_id=batch_id,
            job_id=job_id,
            worker_id=worker_id,
            filename=filename,
            byte_length=byte_length,
        )
    
    def get_bytes(self) -> bytes:
        d = asdict(self)
        if not d:
            raise ProtocolError(
                "images_ready is incomplete."
            )
        return json.dumps(d).encode("utf-8")




@dataclass
class WorkerJob:
    """Internal representation of a job within the worker."""
    input_file: str | None = None
    out_path: str | None = None
    batch_id: int | None = None
    job_id: int | None = None
    options: FileOptions | None = None


    @classmethod
    def from_new_job(
        cls,
        input_file: str,
        out_path: str,
        batch_id: int,
        job_id: int,
        options: FileOptions
    ) -> "WorkerJob":
        """Worker creates from new_job"""
        return cls(
            input_file=input_file,
            out_path=out_path,
            batch_id=batch_id,
            job_id=job_id,
            options=options,
        )


@dataclass
class JobError:
    """Error report from worker to backend."""

    v: int = PROTOCOL_VERSION
    type: Literal["job_error"] = "job_error"
    batch_id: int | None = None
    job_id: int | None = None
    w_id: int | None = None
    stage: JobStage = "unknown"
    error: str = ""
    traceback: str | None = None
    retryable: bool = False

    def error_dict(self):
        d = asdict(self)
        if (self.batch_id is None or self.job_id is None
            or self.w_id is None or self.traceback is None):

            raise ProtocolError(
                "Incomplete JobError Message"
            )
        return d


@dataclass
class Heartbeat:
    """UDP heartbeat from worker to backend."""

    type: Literal["heartbeat"] = "heartbeat"
    worker_id: int | None = None
    time: float | None = None

    def get_heartbeat(self) -> dict[str, Any]:
        if self.worker_id is None:
            raise ProtocolError(
                "Host or port are unspecified"
            )
        self.time = time.time()
        return asdict(self)


@dataclass
class WorkerRegistration:
    """Worker -> Backend registration message."""

    type: Literal["new_convertor"] = "new_convertor"
    host: str | None = None
    port: int | None = None

    def get_reg_dict(self) -> dict:
        d = asdict(self)
        if self.host is None or self.port is None:
            raise ProtocolError(
                "Registration message doesn't have host or port."
            )
        return d


@dataclass
class WorkerAck:
    """Backend -> Worker after backend registers worker"""

    type: Literal["ack"] = "ack"
    id: int | None = None


@dataclass
class NewBatch:
    """Backend -> Worker to clear state for new batch."""
    type: Literal["new_batch"] = "new_batch"
    batch_id: int | None = None
    # Set to false for now to override and clear state, but in
    # the future, can add the ability to finish current jobs
    # while filling up queue for the next jobs
    finish_jobs: bool = False

    def new_batch_dict(self):
        d = asdict(self)
        if self.batch_id is None:
            raise ProtocolError(
                "Unfinished new_batch msg."
            )
        return d


@dataclass
class Shutdown:
    """Shutdown signal."""
    type: Literal["shutdown"] = "shutdown"
    host: str | None = None
    port: int | None = None

    def shutdown_dict(self):
        d = asdict(self)
        if self.host is None or self.port is None:
            raise ProtocolError(
                "Unfinished shutdown msg."
            )
        return d


def parse_file_options(data: dict[str, Any] | None) -> FileOptions:
    if data is None:
        return FileOptions()
    return FileOptions(
        lossless=bool(data.get("lossless", False)),
        text_focus=bool(data.get("text_focus", False)),
        has_text=bool(data.get("has_text", False)),
        type=data.get("type", "default"),
        crop_size_w=data.get("crop_size_w"),
        crop_size_h=data.get("crop_size_h"),
        crop_top_x=data.get("crop_top_x"),
        crop_top_y=data.get("crop_top_y"),
        crop_w=data.get("crop_w"),
        crop_h=data.get("crop_h"),
        size_type=data.get("size_type", "content"),
        width=data.get("width"),
        height=data.get("height"),
    )
```

### packages/shared/src/webp_shared/tcp.py

```python
"""
TCP Networking Utitlities For JobService and Workers

Format:
    [4 bytes: header length big-endian]
    [N bytes: JSON header]
    [M bytes: binary payload (if byte_length is in header)]
"""

from __future__ import annotations

import json
import logging
import socket
import struct
import tempfile
import threading
from pathlib import Path
from typing import Any, Callable

from .files import extract_files
from .protocol import SendFiles

logger = logging.getLogger(__name__)

CONNECT_TIMEOUT = 10.0
RECV_TIMEOUT = 30.0
SEND_TIMEOUT = 30.0


class TCPError(Exception):
    """Base Exception for TCP Operations."""
    pass


class ConnectionFailed(TCPError):
    """Raised when connection can't be established."""
    pass

class SendFailed(TCPError):
    """Raised when sending data fails."""
    pass

class RecvFailed(TCPError):
    """Raised when receiving data fails."""
    pass


def recv_exact(conn: socket.socket, n: int) -> bytes:
    buf = b""
    while len(buf) < n:
        chunk = conn.recv(n - len(buf))
        if not chunk:
            raise RecvFailed(f"Connection closed after {len(buf)}/{n} bytes")
        buf += chunk
    return buf


def send_tcp(host: str, port: int, msg: dict[str, Any]) -> None:
    header_bytes = json.dumps(msg).encode("utf-8")
    prefix = struct.pack(">I", len(header_bytes))
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(CONNECT_TIMEOUT)
            sock.connect((host, port))
            sock.settimeout(SEND_TIMEOUT)
            sock.sendall(prefix)
            sock.sendall(header_bytes)
    except socket.timeout as e:
        raise ConnectionFailed(f"Timeout connecting to {host}:{port}") from e
    except OSError as e:
        raise ConnectionFailed(f"Failed to connect to {host}:{port}: {e}") from e


def send_file_tcp(host: str, port: int, images_ready: SendFiles) -> None:
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(CONNECT_TIMEOUT)    
            s.connect((host, port))
            s.settimeout(SEND_TIMEOUT)
            s.sendall(images_ready.prefix)
            s.sendall(images_ready.header)
            s.sendall(images_ready.file_bytes)

    except socket.timeout as e:
        raise ConnectionFailed(f"Timout connection to {host}:{port}") from e
    except OSError as e:
        raise ConnectionFailed(f"Failed to connect to {host}:{port}: {e}") from e

MessageHandler = Callable[[dict[str, Any]], None]

def tcp_server(
    host: str, 
    port: int, 
    storage_path: Path, 
    shutdown_event: threading.Event, 
    message_handler: MessageHandler,
) -> None:
    """Run TCP server that receives messages and optional file payloads."""

    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        sock.bind((host, port))
        sock.listen(5) 

        sock.settimeout(1.0)

        logger.info("TCP Server listening on %s:%d", host, port)

        while not shutdown_event.is_set():
            try:
                conn, addr = sock.accept()
            except socket.timeout:
                continue
            except OSError as e:
                if shutdown_event.is_set():
                    break
                logger.error("Accept failed %s", e)
                continue

            with conn:
                conn.settimeout(RECV_TIMEOUT)

                try:
                    _handle_connection(conn, addr, storage_path, message_handler)
                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.warning("Invalid message from %s: %s", addr, e)
                except (ConnectionError, socket.timeout) as e:
                    logger.warning("Connection error from %s: %s", addr, e)
                except Exception:
                    logger.exception("Unexpected error handling connection from %s", addr)

        logger.info("TCP Server shutting down")

def _handle_connection(
    conn: socket.socket,
    addr: tuple[str, int],
    storage_path: Path,
    message_handler: MessageHandler,
) -> None:
    header_len_bytes = recv_exact(conn, 4)
    header_len = struct.unpack(">I", header_len_bytes)[0]
    if header_len > 10_000_000:
        raise ValueError(f"Header too large: {header_len} bytes")
    
    header_bytes = recv_exact(conn, header_len)
    header = json.loads(header_bytes.decode("utf-8"))
    msg_type = header.get("type")

    logger.debug("Received %s from %s", msg_type, addr)
    if msg_type in ("new_job", "images_ready"):
        byte_length = header.get("byte_length")
        if byte_length is None or byte_length < 0:
            raise ValueError(f"Invalid byte_length: {byte_length}")

        filename = header.get("filename", "upload.bin")
        filename = Path(filename).name
        payload = recv_exact(conn, byte_length)

        suffix = Path(filename).suffix.lower()
        if suffix in (".jpg", ".jpeg", ".png"):
            out_path = storage_path / filename
            out_path.write_bytes(payload)
            header["saved_path"] = str(out_path)
        elif suffix == ".zip":
            with tempfile.TemporaryDirectory() as temp_dir:
                zip_path = Path(temp_dir) / "tmp.zip"
                zip_path.write_bytes(payload)
                job_id = header.get("job_id", "unknown")
                batch_id = header.get("batch_id", "unknown")
                dest = storage_path / str(batch_id) / str(job_id)
                files = extract_files(zip_path, dest)
                header["paths"] = files
                header["saved_path"] = str(dest)
        else:
            out_path = storage_path / filename
            out_path.write_bytes(payload)
            header["saved_path"] = str(out_path)
        
    message_handler(header)
```

### packages/shared/src/webp_shared/udp.py

```python
"""
UDP networking utilities for monitoring workers' heartbeasts.

Workers send UDP heartbeats to the backend every second. The backend
uses these to detect dead workers and reassign their jobs.
"""

from __future__ import annotations

import json
import logging
import socket
import threading
import time
from typing import Any, Callable
from .protocol import Heartbeat

logger = logging.getLogger(__name__)


def send_udp(host: str, port: int, msg: dict[str, Any]) -> None:
    """Send a JSON message over UDP (fire-and-forget)."""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as sock:
            message = json.dumps(msg).encode("utf-8")
            sock.sendto(message, (host, port))
    except OSError as e:
        logger.debug("UDP send failed to %s:%d: %e", host, port, e)


HeartbeatHandler = Callable[[dict[str, Any]], None]


def send_heartbeats(
    worker_id: int,
    backend_host: str,
    backend_udp_port: int,
    shutdown_event: threading.Event,
    interval: float = 2.0,
) -> None:
    """Worker -> Backend UDP heartbeats every 2.0 seconds default."""
    logger.info(
        "Starting heartbeat sender: %d -> %s:%d",
        worker_id, backend_host, backend_udp_port,
    )
    heartbeat = Heartbeat(
        worker_id=worker_id
    )
    while not shutdown_event.is_set():
        send_udp(
            backend_host, 
            backend_udp_port,
            heartbeat.get_heartbeat()
        )
        shutdown_event.wait(timeout=interval)

    logger.info("Heartbeat sender stopped")


def udp_server(
        host: str,
        port: int,
        shutdown: threading.Event,
        heartbeat_handler: HeartbeatHandler
) -> None:
    """
    Run UDP server as one of JobServices' threads to receive heartbeats and add to
    Heartbeat Queue with heartbeat handler.
    """
    with socket.socket(socket.AF_INET, socket.SOCK_DGRAM) as sock:
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        sock.bind((host, port))
        sock.settimeout(1.0)

        logger.info("UDP server listening on %s:%d", host, port)

        while not shutdown.is_set():
            try:
                data, addr = sock.recvfrom(4096)
            except socket.timeout:
                continue
            except OSError as e:
                if shutdown.is_set():
                    break
                logger.error("UDP recv failed: %s", e)
                continue

            try:
                message = json.loads(data.decode("utf-8"))
                heartbeat_handler(message)
            except json.JSONDecodeError:
                logger.warning("Invalid JSON from %s", addr)
            except Exception:
                logger.exception("Error handling heartbeat from %s", addr)
        
        logger.info("UDP server shutting down.")


                             


```

### pyproject.toml

```python
[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "W"]
ignore = ["E501"]  # Line too long - handled by formatter

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_ignores = true
disallow_untyped_defs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_functions = "test_*"
addopts = "-v"
```

### scripts/run_backend.sh

```python
#!/bin/bash
# Run the WebPForge backend server

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
VENV_DIR="$PROJECT_ROOT/.venv"

# Activate virtual environment
if [ -f "$VENV_DIR/bin/activate" ]; then
    source "$VENV_DIR/bin/activate"
else
    echo "Error: Virtual environment not found at $VENV_DIR"
    echo "Run ./scripts/setup/install_all.sh first"
    exit 1
fi

# Change to backend directory (so relative paths work)
cd "$PROJECT_ROOT/apps/backend"

echo "Starting WebPForge backend on http://127.0.0.1:5000"
echo "Press Ctrl+C to stop"
echo ""

# Run the backend
python -m webp_backend.app
```

### scripts/run_frontend.sh

```python
#!/bin/bash
# Run the WebPForge frontend development server

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
FRONTEND_DIR="$PROJECT_ROOT/apps/frontend"

cd "$FRONTEND_DIR"

# Check if node_modules exists
if [ ! -d "node_modules" ]; then
    echo "Installing frontend dependencies..."
    npm install
fi

echo "Starting WebPForge frontend on http://localhost:5173"
echo "API requests will be proxied to http://127.0.0.1:5000"
echo "Press Ctrl+C to stop"
echo ""

npm run dev
```

### scripts/run_worker.sh

```python
#!/bin/bash
# Run a WebPForge worker

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
VENV_DIR="$PROJECT_ROOT/.venv"

# Activate virtual environment
if [ -f "$VENV_DIR/bin/activate" ]; then
    source "$VENV_DIR/bin/activate"
else
    echo "Error: Virtual environment not found at $VENV_DIR"
    echo "Run ./scripts/setup/install_all.sh first"
    exit 1
fi

# Check for cwebp
if ! command -v cwebp &> /dev/null; then
    echo "Error: cwebp is not installed"
    echo "Install with: sudo apt install webp"
    exit 1
fi

echo "Starting WebPForge worker..."
echo "Backend: 127.0.0.1:5055 (TCP), 127.0.0.1:5056 (UDP)"
echo "Press Ctrl+C to stop"
echo ""

# Run the worker with verbose logging
webp-worker --verbose "$@"
```

### scripts/setup/install_all.sh

```python
#!/bin/bash
# WebPForge - Complete Installation Script
# This script sets up a single virtual environment for the entire monorepo

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

echo "============================================"
echo "WebPForge - Installation"
echo "============================================"
echo "Project root: $PROJECT_ROOT"
echo ""

# Create and activate virtual environment
VENV_DIR="$PROJECT_ROOT/.venv"

if [ ! -d "$VENV_DIR" ]; then
    echo "Creating virtual environment..."
    python3 -m venv "$VENV_DIR"
fi

echo "Activating virtual environment..."
source "$VENV_DIR/bin/activate"

# Upgrade pip
echo "Upgrading pip..."
pip install --upgrade pip

# Install packages in dependency order (editable mode)
echo ""
echo "Installing shared package..."
pip install -e "$PROJECT_ROOT/packages/shared"

echo ""
echo "Installing converter package..."
pip install -e "$PROJECT_ROOT/packages/converter"

echo ""
echo "Installing backend..."
pip install -e "$PROJECT_ROOT/apps/backend"

echo ""
echo "Installing worker..."
pip install -e "$PROJECT_ROOT/apps/worker"

# Install dev dependencies
echo ""
echo "Installing development dependencies..."
pip install pytest ruff mypy

echo ""
echo "============================================"
echo "Python setup complete!"
echo "============================================"
echo ""
echo "Virtual environment: $VENV_DIR"
echo ""
echo "To activate the virtual environment:"
echo "  source $VENV_DIR/bin/activate"
echo ""

# Check for cwebp
if command -v cwebp &> /dev/null; then
    echo "✓ cwebp is installed"
else
    echo "⚠ cwebp is NOT installed"
    echo "  Install with: sudo apt install webp"
fi

echo ""
echo "Next steps:"
echo "  1. Install frontend: cd apps/frontend && npm install"
echo "  2. Run backend: ./scripts/run_backend.sh"
echo "  3. Run worker: ./scripts/run_worker.sh"
echo "  4. Run frontend: ./scripts/run_frontend.sh"
```

### scripts/setup/install_backend.sh

```python

```

### scripts/setup/install_converter.sh

```python

```

### scripts/setup/install_frontend.sh

```python

```

### scripts/setup/install_shared.sh

```python

```

### scripts/setup/install_worker.sh

```python

```

### tests/__init__.py

```python

```

### tests/integration/test_job_flow.py

```python

```

### tests/unit/test_analysis.py

```python

```

### tests/unit/test_cwebp.py

```python

```

### tests/unit/test_protocol.py

```python

```

